{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad86608",
   "metadata": {},
   "source": [
    "## 1. What is Ensemble Learning in machine learning?\n",
    "Ensemble Learning is a technique where multiple models are combined to improve predictive performance. The main idea is that combining several weak or diverse learners results in a more accurate and robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0346695d",
   "metadata": {},
   "source": [
    "## 2. Difference between Bagging and Boosting\n",
    "Bagging reduces variance by training models independently on different bootstrap samples, whereas Boosting reduces bias by training models sequentially and focusing on misclassified instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a70039",
   "metadata": {},
   "source": [
    "## 3. Bootstrap sampling and its role in Bagging\n",
    "Bootstrap sampling creates multiple datasets by sampling with replacement. In Bagging, it provides different training sets to each model, increasing diversity and reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a6ae4",
   "metadata": {},
   "source": [
    "## 4. Out-of-Bag (OOB) samples and OOB score\n",
    "OOB samples are data points not included in a model's bootstrap sample. OOB score estimates model performance using these samples without needing a separate validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e18388",
   "metadata": {},
   "source": [
    "## 5. Feature importance: Decision Tree vs Random Forest\n",
    "A single Decision Tree may give unstable feature importance due to overfitting, while Random Forest averages importance across trees, resulting in more reliable estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8b335",
   "metadata": {},
   "source": [
    "# 6. Random Forest Classifier (Breast Cancer Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97cdfc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst area              0.139357\n",
      "worst concave points    0.132225\n",
      "mean concave points     0.107046\n",
      "worst radius            0.082848\n",
      "worst perimeter         0.080850\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "importances = pd.Series(model.feature_importances_, index=data.feature_names)\n",
    "print(importances.sort_values(ascending=False).head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493723d",
   "metadata": {},
   "source": [
    "# 7. Bagging Classifier using Decision Tree (Iris Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff38343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "base_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=base_tree,\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Bagging Classifier Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b753571",
   "metadata": {},
   "source": [
    "# 8. Random Forest Classifier Hyperparameter Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6d6890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'n_estimators': 50}\n",
      "Accuracy: 0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "param_grid = {'n_estimators': [50, 100], 'max_depth': [None, 5, 10]}\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid = GridSearchCV(rf, param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best = grid.best_estimator_\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, best.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f91ff",
   "metadata": {},
   "source": [
    "## 9. Bagging vs Random Forest Regressor (California Housing Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28175e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karka\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor MSE: 0.2579153056796594\n",
      "Random Forest Regressor MSE: 0.25638991335459355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "bag = BaggingRegressor(\n",
    "    base_estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Bagging Regressor MSE:\", mean_squared_error(y_test, bag.predict(X_test)))\n",
    "print(\"Random Forest Regressor MSE:\", mean_squared_error(y_test, rf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef16dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor MSE: 0.2579153056796594\n",
      "Random Forest Regressor MSE: 0.25638991335459355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = fetch_california_housing(download_if_missing=True)\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "bagging_model = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(random_state=42),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bagging_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "bag_pred = bagging_model.predict(X_test)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "bag_mse = mean_squared_error(y_test, bag_pred)\n",
    "rf_mse = mean_squared_error(y_test, rf_pred)\n",
    "\n",
    "print(\"Bagging Regressor MSE:\", bag_mse)\n",
    "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
