{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f976bb05",
   "metadata": {},
   "source": [
    "# 1. What is Information Gain, and how is it used in Decision Trees?\n",
    "Information Gain measures how much uncertainty (entropy) is reduced after splitting a dataset on a feature. In decision trees, the feature with the highest information gain is chosen for splitting because it best separates the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc432a",
   "metadata": {},
   "source": [
    "# 2. What is the difference between Gini Impurity and Entropy?\n",
    "Gini Impurity measures the probability of incorrect classification, while Entropy measures the level of disorder in data. Gini is computationally faster, while Entropy is based on information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dda7e1",
   "metadata": {},
   "source": [
    "# 3. What is Pre-Pruning in Decision Trees?\n",
    "Pre-pruning stops the growth of a decision tree early by setting conditions like maximum depth or minimum samples per node, which helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc2ff6",
   "metadata": {},
   "source": [
    "# 4. Decision Tree Classifier using Gini Impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cea4b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances: [0.01333333 0.01333333 0.05072262 0.92261071]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Feature Importances:\", model.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0636f8",
   "metadata": {},
   "source": [
    "# 5. What is a Support Vector Machine (SVM)?\n",
    "SVM is a supervised learning algorithm that finds the optimal hyperplane which best separates data points of different classes by maximizing the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4cb11",
   "metadata": {},
   "source": [
    "# 6. What is the Kernel Trick in SVM?\n",
    "The kernel trick allows SVMs to transform data into a higher-dimensional space without explicitly computing it, enabling classification of non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2cc14d",
   "metadata": {},
   "source": [
    "# 7. SVM Classifiers with Linear and RBF Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f07541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel Accuracy: 0.9814814814814815\n",
      "RBF Kernel Accuracy: 0.7592592592592593\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "svm_linear = SVC(kernel='linear')\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "\n",
    "svm_linear.fit(X_train, y_train)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "pred_linear = svm_linear.predict(X_test)\n",
    "pred_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "print(\"Linear Kernel Accuracy:\", accuracy_score(y_test, pred_linear))\n",
    "print(\"RBF Kernel Accuracy:\", accuracy_score(y_test, pred_rbf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b93734c",
   "metadata": {},
   "source": [
    "# 8. What is the Na誰ve Bayes classifier, and why is it called 'Naive'?\n",
    "Na誰ve Bayes is a probabilistic classifier based on Bayes' Theorem. It is called na誰ve because it assumes that features are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f165fc1",
   "metadata": {},
   "source": [
    "# 9 Differences between Gaussian, Multinomial, and Bernoulli Naive bayes\n",
    "Gaussian NB is used for continuous data, Multinomial NB for count-based data like text, and Bernoulli NB for binary features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3063cc",
   "metadata": {},
   "source": [
    "# 10. Gaussian Na誰ve Bayes on Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aebb0a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9415204678362573\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
